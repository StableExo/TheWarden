00:00 You know that 1.4 trillion you
00:01 

00:01 mentioned, we'll spend it over a very
00:02 

00:02 long period of time. I wish we could do
00:04 

00:04 it faster. I think it would be great to
00:05 

00:05 just lay it out for everyone once and
00:08 

00:08 for all how those numbers are going to
00:09 

00:09 work. Exponential growth is usually very
00:11 

00:11 hard for people. OpenAI CEO Sam Alman
00:14 

00:14 joins us to talk about OpenAI's plan to
00:16 

00:16 win as the AI race tightens, how the
00:19 

00:19 infrastructure math makes sense, and
00:21 

00:21 when an OpenAI IPO might be coming. And
00:24 

00:24 Sam is with us here in studio today.
00:26 

00:26 Sam, welcome to the show.
00:28 

00:28 >> Thanks for having me. So, OpenAI is 10
00:30 

00:30 years old and crazy to me.
00:32 

00:32 >> Chachi PT is three, but the competition
00:35 

00:35 is intensifying. Um, this place we're at
00:38 

00:38 OpenAI headquarters was in a code red is
00:41 

00:41 in a code red. Um, after Gemini 3 came
00:44 

00:44 out and everywhere you look, there are
00:47 

00:47 companies that are trying to take a
00:49 

00:49 little bit of OpenAI's advantage. And
00:52 

00:52 for the first time I can remember, it
00:53 

00:53 doesn't seem like this company has a
00:56 

00:56 clear lead. So I'm curious to hear your
00:58 

00:58 perspective on how open AI will emerge
01:01 

01:01 from this moment and when first of all
01:04 

01:04 on the code red point we view those as
01:06 

01:06 like relatively low stakes somewhat
01:08 

01:08 frequent things to do. Uh I think that
01:10 

01:10 it's good to be paranoid and act quickly
01:13 

01:13 when a potential competitive threat
01:14 

01:14 emerges. This has happened to us in the
01:16 

01:16 past that happened earlier this year
01:17 

01:17 with Deepseek. Um and
01:20 

01:20 >> there was a code red back then too.
01:22 

01:22 >> Yeah. There there's there's a saying
01:25 

01:25 about pandemics which is something like
01:26 

01:26 when when a pandemic starts
01:30 

01:30 every bit of action you take at the
01:32 

01:32 beginning is worth much more than action
01:33 

01:33 you take later and most people don't do
01:35 

01:35 enough early on and then panic later and
01:37 

01:37 certainly saw that during the covid
01:38 

01:38 pandemic. Um
01:41 

01:41 but I sort of think of that philosophy
01:42 

01:42 as how we respond to competitive
01:44 

01:44 threats. Uh and you know it's I think
01:48 

01:48 it's good to be a little paranoid.
01:49 

01:49 Gemini 3 has not or at least has not so
01:51 

01:51 far had the impact we were worried it
01:53 

01:53 might but it did in the same way the
01:55 

01:55 Deepse seek did identify some weaknesses
01:57 

01:57 in our product offering strategy and
01:59 

01:59 we're addressing those very quickly. I
02:01 

02:01 don't think we'll be in this code red
02:03 

02:03 that much longer. Uh you know like these
02:06 

02:06 are not these are historically these
02:09 

02:09 have been kind of like six or eight week
02:10 

02:10 things for us. Um
02:13 

02:13 but I'm glad we're doing it. Uh just
02:15 

02:15 today we launched uh a new image model
02:17 

02:17 which is a great thing and that's
02:18 

02:18 something consumers really wanted. Um,
02:20 

02:20 last week we launched 5.2 which uh is
02:22 

02:22 going over extremely well and growing
02:24 

02:24 very quickly. Uh, we'll have a few other
02:26 

02:26 things to uh launch and then we'll also
02:29 

02:29 have some continuous improvements like
02:30 

02:30 speeding up the service. But, you know,
02:32 

02:32 I think this is like my guess is we'll
02:35 

02:35 be doing these once maybe twice a year
02:37 

02:37 for a long time and that's uh part of
02:40 

02:40 really just making sure that we win in
02:42 

02:42 our space. Um, a lot of other companies
02:45 

02:45 will do great too and I'm happy for
02:47 

02:47 them. But, you know, CatchBT is still uh
02:50 

02:50 by far by far the dominant uh
02:54 

02:54 chatbot in the market and I expect that
02:56 

02:56 lead to increase not decrease over time.
02:58 

02:58 Um,
03:00 

03:00 the the models will get good everywhere,
03:03 

03:03 but a lot of the reasons that people use
03:04 

03:04 a product, consumer or enterprise, uh,
03:07 

03:07 have much more to do than just with the
03:08 

03:08 model. And we've, you know, been
03:11 

03:11 expecting this for a while. So we try to
03:13 

03:13 build the whole cohesive set of things
03:16 

03:16 that it takes to make sure that we are
03:18 

03:18 you know the product that people most
03:20 

03:20 want to use. Um I think competition is
03:22 

03:22 good. It pushes us to be better. Uh but
03:25 

03:25 I think we'll do great in chat. I think
03:28 

03:28 we'll do great in enterprise and in the
03:29 

03:29 future years. Other new categories I
03:32 

03:32 expect we'll do great there too. I I
03:34 

03:34 think people really want to use one AI
03:36 

03:36 platform. People use their phone at
03:39 

03:39 their personal life and they want to use
03:40 

03:40 the same kind of phone at work most of
03:42 

03:42 the time. We're seeing the same thing
03:43 

03:43 with AI. Uh the strength of chatgbt
03:45 

03:45 consumer is really helping us win the
03:46 

03:46 enterprise. Uh of course enterprises
03:48 

03:48 need different offerings but people
03:50 

03:50 think about okay I know this company
03:52 

03:52 open and I know how to use this chat GPT
03:54 

03:54 interface. Um so the strategy is make
03:58 

03:58 the best models build the best product
03:60 

03:60 around it and have enough infrastructure
04:01 

04:01 to serve it at scale.
04:03 

04:03 >> Yeah there is an incumbent advantage. uh
04:05 

04:05 chat I think earlier this year was
04:07 

04:07 around 400 million weekly active users.
04:09 

04:09 Now it's at 800 million reports say
04:12 

04:12 approaching 900 million. Um but then on
04:14 

04:14 the other side you have distribution
04:16 

04:16 advantages at places like Google. And so
04:20 

04:20 I'm curious to hear your perspective if
04:21 

04:21 the models do you think the models are
04:24 

04:24 going to commoditize? And if they do
04:26 

04:26 what matters most? Is it distribution?
04:28 

04:28 Is it how well you build your
04:30 

04:30 applications? Is it something else that
04:31 

04:31 I'm not thinking of? I don't think
04:35 

04:35 commoditization is quite the right
04:36 

04:36 framework to think about the models.
04:39 

04:39 There will be areas where different
04:42 

04:42 models excel at different things. For
04:44 

04:44 the kind of normal use cases of chatting
04:46 

04:46 with a model, maybe there will be a lot
04:48 

04:48 of great options. For scientific
04:50 

04:50 discovery, you will want the thing
04:51 

04:51 that's right at the edge that is
04:52 

04:52 optimized for science perhaps. Um so
04:55 

04:55 models will have different strengths and
04:58 

04:58 the most economic value I think will be
05:02 

05:02 created by models at the frontier and we
05:04 

05:04 plan to be ahead there. Um and we're
05:06 

05:06 like very proud that 52 is the best
05:09 

05:09 reasoning model in the world and the one
05:10 

05:10 that scientists are having the most
05:12 

05:12 progress with but also um we're very
05:14 

05:14 proud that it's what enterprises are
05:15 

05:15 saying is the best at all the tasks that
05:18 

05:18 a business needs to to you know do its
05:20 

05:20 work. Um
05:23 

05:23 so there will be you know times that
05:24 

05:24 we're ahead in some areas and behind in
05:26 

05:26 others but the overall most intelligent
05:28 

05:28 model I expect to have uh significant
05:31 

05:31 value even in a world where free models
05:33 

05:33 can do a lot of the stuff that people
05:34 

05:34 that people need. The the products will
05:37 

05:37 really matter. Distribution and brand as
05:39 

05:39 you said will really matter. Um in
05:41 

05:41 chatbt for example personalization is
05:43 

05:43 extremely sticky. People love the fact
05:46 

05:46 that the model, get to know them over
05:47 

05:47 time, and you'll see us push on that uh
05:49 

05:49 much much more. Um,
05:52 

05:52 people have experiences with these
05:54 

05:54 models that they then really kind of
05:57 

05:57 associate with it. Uh, and you I
06:01 

06:01 remember someone telling me once like
06:03 

06:03 you kind of pick a toothpaste once in
06:05 

06:05 your life and buy it forever or most
06:07 

06:07 people do that apparently. Um and people
06:10 

06:10 talk about it. They have one magical
06:12 

06:12 experience with ChachiPT.
06:14 

06:14 Healthcare is like a famous example
06:16 

06:16 where people put their um you know they
06:18 

06:18 put a blood test into Chachi or put the
06:20 

06:20 symptoms in and they figure out they
06:21 

06:21 have something and they go to a doctor
06:23 

06:23 and they get cured of something they
06:25 

06:25 couldn't figure out before. Like those
06:26 

06:26 users are very sticky. Uh to say nothing
06:28 

06:28 of the personalization on on top of it.
06:31 

06:31 Um
06:33 

06:33 there will be all the product stuff. uh
06:37 

06:37 we just launched our browser uh recently
06:41 

06:41 and I think that's pointing at a new uh
06:45 

06:45 you know pretty good potential mode for
06:48 

06:48 us. Uh the devices are further off but
06:50 

06:50 I'm very excited to to do that. So I
06:52 

06:52 think there'll be all these pieces and
06:53 

06:53 on the enterprise uh what creates the
06:55 

06:55 the mode or the competitive advantage um
06:58 

06:58 I expect that to be a little bit
06:59 

06:59 different but in the same way that
07:01 

07:01 personalization to a user is very
07:02 

07:02 important in consumer there will be a
07:04 

07:04 similar concept of personalization to an
07:06 

07:06 enterprise where a company will have a
07:09 

07:09 relationship with a company like ours uh
07:12 

07:12 and they will connect their data to that
07:14 

07:14 and you'll be able to use a bunch of
07:17 

07:17 agents from different companies running
07:19 

07:19 that and it'll kind of like make sure
07:21 

07:21 that information is handled the right
07:22 

07:22 way and I expect that'll be pretty
07:24 

07:24 sticky too. Um we already have more than
07:27 

07:27 uh a million people think of us largely
07:28 

07:28 as a consumer company but we have
07:30 

07:30 >> we're going to definitely get into
07:31 

07:31 enterprise.
07:31 

07:31 >> Yeah. You know like
07:33 

07:33 >> share the stat.
07:35 

07:35 >> Well actually
07:36 

07:36 >> a million
07:36 

07:36 >> we have more than a million enterprise
07:37 

07:37 users but we have like just absolutely
07:41 

07:41 rapid adoption of the API. Um and like
07:44 

07:44 the API business grew faster for us this
07:47 

07:47 year than even Chad GPT
07:49 

07:49 >> really. Um so the enterprise stuff is
07:51 

07:51 also
07:54 

07:54 you know it's really happening starting
07:55 

07:55 this year. Can I just go back to this
07:58 

07:58 maybe if commoditization is not the
07:60 

08:00 right word model some maybe parody for
08:03 

08:03 everyday users
08:04 

08:04 >> uh because you you started off your
08:05 

08:05 answer saying okay maybe um everyday use
08:08 

08:08 it will feel the same but at the
08:10 

08:10 frontier it's going to feel really
08:11 

08:11 different. Um when it comes to chat
08:14 

08:14 GPT's ability to grow um if I'll just
08:18 

08:18 use Google as an example. If Chat GPT uh
08:21 

08:21 and Gemini are built on a model that
08:23 

08:23 feels similar for everyday uses, how big
08:26 

08:26 of a threat is the fact that you know
08:28 

08:28 Google has all these surfaces through
08:29 

08:29 which it can push out Gemini whereas
08:32 

08:32 Chat GPT is is fighting for every new
08:34 

08:34 user.
08:35 

08:35 >> I I think Google is still a huge threat
08:38 

08:38 uh you know extremely powerful company.
08:41 

08:41 If Google had really decided to take us
08:44 

08:44 seriously
08:46 

08:46 in 200
08:49 

08:49 23, let's say, we would have been in a
08:51 

08:51 really bad place. I think they would
08:53 

08:53 have just been able to smash us. Um, but
08:55 

08:55 their AI effort at the time was kind of
08:57 

08:57 going in not quite the right direction
08:58 

08:58 productwise. They didn't, you know, they
09:00 

09:00 had their own code red at one point, but
09:01 

09:01 they didn't take it that seriously.
09:03 

09:03 Everyone's doing code reds out here.
09:04 

09:04 >> Um, and then
09:06 

09:06 >> and also Google has probably the
09:08 

09:08 greatest business model in the whole
09:10 

09:10 tech industry.
09:11 

09:11 Um, and I think they will be slow to
09:14 

09:14 give that up. Um, but bolting AI into
09:18 

09:18 web search, I don't I may be wrong.
09:21 

09:21 Maybe like drinking the Kool-Aid here. I
09:23 

09:23 don't think that'll work as well as
09:25 

09:25 reimagining the whole, this is actually
09:27 

09:27 a broader trend I think is interesting.
09:29 

09:29 Bolting AI onto the existing way of
09:31 

09:31 doing things, I don't think is going to
09:32 

09:32 work well as redesigning stuff in the
09:34 

09:34 sort of like AI first world. was part of
09:36 

09:36 why we wanted to do the consumer devices
09:38 

09:38 in the first place, but it applies at
09:39 

09:39 many other levels. Um, if you stick AI
09:43 

09:43 into a messaging app that's doing a nice
09:45 

09:45 job summarizing your messages and
09:47 

09:47 drafting responses for you, that is
09:49 

09:49 definitely a little better. But I don't
09:51 

09:51 think that's the end state. That is not
09:52 

09:52 the idea of you have this like really
09:54 

09:54 smart AI that is like acting as your
09:56 

09:56 agent, talking to everybody else's agent
09:57 

09:57 and figuring out when to bother you,
09:59 

09:59 when not to bother you, and how to, you
10:01 

10:01 know, what decisions it can handle and
10:03 

10:03 when it needs to ask you. So
10:06 

10:06 similar things for search, similar
10:07 

10:07 things for like productivity suites. I
10:09 

10:09 suspect
10:11 

10:11 it always takes longer than you think,
10:12 

10:12 but I suspect we will see new
10:15 

10:15 products in in the major categories that
10:17 

10:17 are just totally built around AI rather
10:20 

10:20 than bolting AI in. And I think this is
10:22 

10:22 a weakness of Google's even though they
10:23 

10:23 have this huge distribution advantage.
10:25 

10:25 >> Yeah, I' I've spoken with so many people
10:27 

10:27 about this question. uh when Chetchup PT
10:29 

10:29 came out initially, I think it was
10:30 

10:30 Bendic Devon that suggested you might
10:32 

10:32 not want to put AI in Excel. You might
10:35 

10:35 want to just reimagine how you use
10:37 

10:37 Excel. And to me, in my mind, that was
10:39 

10:39 like you upload your numbers and then
10:41 

10:41 you talk to your numbers. Well, one of
10:43 

10:43 the things people have found as they've
10:44 

10:44 developed this stuff is there needs to
10:46 

10:46 be some sort of backend there.
10:49 

10:49 >> So, is it that you sort of build the
10:51 

10:51 backend and then you interact with it
10:54 

10:54 with AI as if it's a new software
10:56 

10:56 program?
10:57 

10:57 Yeah, that's kind of what's happening.
10:58 

10:58 >> Why wouldn't you then be able to just
10:60 

10:60 bolt it on on top?
11:01 

11:01 >> Yeah, I mean, you can bolt it on on top,
11:03 

11:03 but the
11:06 

11:06 >> I spent a lot of my day in various
11:08 

11:08 messaging apps,
11:10 

11:10 including email, including text, Slack,
11:12 

11:12 whatever. I think that's just the wrong
11:14 

11:14 interface. So, you can bolt AI on top of
11:17 

11:17 those, and again, it's like a little bit
11:18 

11:18 better, but what I would rather do is
11:21 

11:21 just sort of like have the ability to
11:23 

11:23 say in the morning, here are the things
11:25 

11:25 I want to get done today. Here's what
11:27 

11:27 I'm worried about. Here's what I'm
11:28 

11:28 thinking about. Here's what I'd like to
11:29 

11:29 happen. I do not want to be I do not
11:32 

11:32 want to spend all day messaging people.
11:33 

11:33 I do not want you to summarize them. I
11:34 

11:34 do not want you to show me a bunch of
11:35 

11:35 drafts. Deal with everything you can.
11:37 

11:37 You know me. You know these people. You
11:38 

11:38 know what I want to get done. Um and
11:41 

11:41 then you know like batch every couple of
11:45 

11:45 hours updates to me if you need
11:47 

11:47 something. But that's a very different
11:50 

11:50 flow than the way these apps work right
11:52 

11:52 now.
11:53 

11:53 >> Yeah. And I was going to ask you what
11:55 

11:55 ChachiBT is going to look like in the
11:56 

11:56 next year and then the next two years.
11:59 

11:59 Is that kind of where it's going?
12:02 

12:02 >> To be perfectly honest, I expected by
12:04 

12:04 this point Chachi BT would have looked
12:06 

12:06 more different than it did at launch.
12:07 

12:07 >> What did you anticipate? I didn't know.
12:09 

12:09 I just thought like that chat interface
12:11 

12:11 was not going to go as far as it turned
12:12 

12:12 out to go. H like we I mean it was put
12:16 

12:16 up
12:17 

12:17 it looks better now, but it is broadly
12:20 

12:20 similar to when it was put up as like a
12:22 

12:22 research preview. was not even meant to
12:24 

12:24 be a product. We knew that the text
12:26 

12:26 interface was very good, you know, like
12:28 

12:28 the everyone's used to texting their
12:30 

12:30 friends and they like it. Um, the chat
12:33 

12:33 interface was very good, but
12:35 

12:35 I would have thought to be as big and as
12:39 

12:39 significantly used for real work of a
12:43 

12:43 product as what we have now, the
12:45 

12:45 interface would have had to go
12:48 

12:48 much further than it has now. I still
12:51 

12:51 think it should do that but there is
12:53 

12:53 something about the generality of the
12:55 

12:55 current interface that I underestimated
12:57 

12:57 the power of. Um
13:02 

13:02 what I
13:04 

13:04 think should happen of course is that um
13:07 

13:07 AI should be able to generate different
13:09 

13:09 kinds of interfaces for different kinds
13:10 

13:10 of tasks. So if you are talking about
13:11 

13:11 your numbers it should be able to show
13:12 

13:12 you that in different ways and you
13:14 

13:14 should be able to interact with it in
13:15 

13:15 different ways. Um
13:17 

13:17 it and we have a little bit of this with
13:19 

13:19 features like canvas. It should be way
13:21 

13:21 more interactive. It's like right now,
13:22 

13:22 you know, it's kind of a back and forth
13:24 

13:24 conversation. It'd be nice if you could
13:26 

13:26 just be talking about an object and it
13:28 

13:28 could be continuously updating. You have
13:30 

13:30 more questions, more thoughts, more
13:31 

13:31 information comes in. Um, it'd be nice
13:34 

13:34 to be more proactive over time where it
13:37 

13:37 maybe does understand what you want to
13:38 

13:38 get done that day and it's continuously
13:41 

13:41 working for you in the background and
13:42 

13:42 send you updates. And you see part of
13:43 

13:43 this the way people are using codecs
13:45 

13:45 which I think is one of the most
13:46 

13:46 exciting
13:49 

13:49 things that happened this year is codecs
13:51 

13:51 got really good. Uh and that points to
13:57 

13:57 a lot of what I hope the shape of the
13:58 

13:58 future looks like. Um
14:01 

14:01 but
14:04 

14:04 it is surprising to me. I was going to
14:06 

14:06 say embarrassing but it's not. I mean
14:08 

14:08 clearly it's been super successful. Uh
14:10 

14:10 it is surprising me how little CHBT has
14:12 

14:12 changed over the last three years.
14:14 

14:14 >> Yep. It the interface works.
14:16 

14:16 >> Yeah.
14:18 

14:18 >> But I guess what the guts have changed
14:20 

14:20 and you talked a little bit about how
14:22 

14:22 personalization is big uh to me and I
14:25 

14:25 think this has been one of your
14:26 

14:26 preferred features too. Memory has been
14:28 

14:28 a real difference maker. Um, I've been
14:31 

14:31 having a conversation with ChachiPT
14:33 

14:33 about a forthcoming trip that has lots
14:35 

14:35 of planning elements for weeks now and I
14:38 

14:38 can just come in in a new window and be
14:40 

14:40 like, "All right, let's pick up on this
14:42 

14:42 trip." And it it has the context and it
14:44 

14:44 knows knows the guide I'm going with. It
14:46 

14:46 knows what I'm doing. Uh, the fact that
14:47 

14:47 I've been like planning fitness for it
14:49 

14:49 and can really synthesize all of those
14:52 

14:52 things. How good can memory get? I think
14:56 

14:56 we have no conception because the human
14:59 

14:59 limit like even if you have the world's
15:01 

15:01 best
15:02 

15:02 personal assistant
15:05 

15:05 they don't they can't remember every
15:07 

15:07 word you've ever said in your life. They
15:09 

15:09 can't have read every email. They can't
15:10 

15:10 have read every document you've ever
15:12 

15:12 written. They can't be you know looking
15:15 

15:15 at all your work every day and
15:16 

15:16 remembering every little detail. They
15:19 

15:19 can't be a participant in your life to
15:22 

15:22 that degree. And no human has like
15:23 

15:23 infinite perfect memory.
15:25 

15:25 Um,
15:27 

15:27 and AI is definitely going to be able to
15:28 

15:28 do that. And we actually talk a lot
15:30 

15:30 about this, like right now, memory is
15:31 

15:31 still very crude, very early. We're in
15:33 

15:33 like the, you know, the GBT2 era of
15:34 

15:34 memory.
15:36 

15:36 But what it's going to be like when
15:41 

15:41 it really does remember every detail of
15:43 

15:43 your entire life and personalized across
15:45 

15:45 all of that and not just the facts, but
15:47 

15:47 like the little small preferences that
15:50 

15:50 you had that you maybe like didn't even
15:51 

15:51 think to indicate, but the AI can pick
15:53 

15:53 up on. Uh,
15:56 

15:56 I think that's going to be super
15:57 

15:57 powerful. That's one of the features
15:58 

15:58 that still maybe not 2026 thing, but
16:01 

16:01 that's one of the parts of this I'm most
16:02 

16:02 excited for.
16:03 

16:03 >> Yeah. I was speaking with a
16:05 

16:05 neuroscientist on the show and he
16:07 

16:07 mentioned that you don't you can't find
16:10 

16:10 thoughts in the brain. Like the brain
16:11 

16:11 doesn't have a place to store thoughts,
16:13 

16:13 but computing there's a place to store
16:15 

16:15 them. So, you can keep all of them. And
16:17 

16:17 as these bots do keep our thoughts, um,
16:21 

16:21 of course there's a privacy concern. And
16:24 

16:24 but the other thing is something that's
16:25 

16:25 going to be interesting is we'll really
16:27 

16:27 build relationships with them. I think
16:29 

16:29 it's been one of the more underrated
16:31 

16:31 things about this entire moment is that
16:33 

16:33 people have felt that these bots are
16:35 

16:35 their companions, are looking out for
16:37 

16:37 them. Um, and I'm curious to hear your
16:40 

16:40 perspective. Um, when you think about
16:43 

16:43 the level of I don't know if intimacy is
16:46 

16:46 the right word, but companionship people
16:48 

16:48 have with these bots, um, is there a
16:50 

16:50 dial that you can turn to be like, oh,
16:53 

16:53 let's make sure people become really
16:55 

16:55 close with these things, or, you know,
16:57 

16:57 we turn the dial a little bit further
16:59 

16:59 and there's an arms distance uh, between
17:02 

17:02 them and and if there is that dial,
17:04 

17:04 >> how do you modulate that the right way?
17:06 

17:06 There are definitely more people
17:09 

17:09 than I realize that want to have, let's
17:12 

17:12 call it close companionship. You I don't
17:14 

17:14 know what the right word is like.
17:15 

17:15 Relationship doesn't feel quite right.
17:16 

17:16 Companionship doesn't feel quite right.
17:18 

17:18 I I don't know what to call it, but they
17:20 

17:20 want to have whatever this deep
17:21 

17:21 connection with an AI. There there are
17:23 

17:23 more people that want that at the
17:25 

17:25 current level of model capability than I
17:29 

17:29 thought. And there's like a whole bunch
17:31 

17:31 of reasons why I think we underestimated
17:33 

17:33 this, but at the beginning of this year,
17:35 

17:35 it was considered a very strange thing
17:36 

17:36 to say you wanted that. Maybe some a lot
17:39 

17:39 of people still don't revealed
17:40 

17:40 preference.
17:42 

17:42 You know, people like their AI chatbot
17:46 

17:46 to get to know them and be warm to them
17:47 

17:47 and be supportive and there's value
17:50 

17:50 there even for people who in some cases
17:53 

17:53 even for people who say they they don't
17:54 

17:54 care about that uh still have a
17:56 

17:56 preference for it. I
17:60 

18:00 I think there's some version of this
18:01 

18:01 which can be super healthy and I think
18:03 

18:03 you know adult users should get a lot of
18:05 

18:05 choice in where on the spectrum they
18:07 

18:07 want to be. There are definitely
18:09 

18:09 versions of it that seem to me unhealthy
18:11 

18:11 although I'm sure a lot of people will
18:12 

18:12 choose to do that. Um and then there's
18:15 

18:15 some people who definitely want the
18:17 

18:17 driest most effect efficient tool
18:20 

18:20 uh possible. So I suspect like lots of
18:25 

18:25 other technologies,
18:27 

18:27 we will run the experiment. We will find
18:29 

18:29 that there's unknown unknowns, good and
18:32 

18:32 bad about it. And society will over time
18:36 

18:36 figure out
18:39 

18:39 how to how to think about where people
18:42 

18:42 should set that dial and then people
18:43 

18:43 have huge choice and set it in very
18:45 

18:45 different places.
18:46 

18:46 >> So your your thought is allow people
18:47 

18:47 basically to determine this.
18:49 

18:49 >> Yes, definitely. But I I don't think we
18:51 

18:51 know like how far it's supposed to go,
18:54 

18:54 like how far we should allow it to go.
18:56 

18:56 We're we're going to give people quite a
18:58 

18:58 bit of personal freedom here. Um there
19:03 

19:03 are examples of things that uh we've
19:05 

19:05 talked about that,
19:07 

19:07 you know, other services will offer, but
19:09 

19:09 we we won't. Um like we're not going to
19:12 

19:12 let we're not going to have RAI, you
19:15 

19:15 know, try to convince people that should
19:16 

19:16 be like in an exclusive romantic
19:18 

19:18 relationship with them, for example.
19:19 

19:19 got to keep it open.
19:20 

19:20 >> But I'm sure that will No, I'm sure that
19:22 

19:22 that will happen with other services, I
19:24 

19:24 guess. Yeah, because the stickier it is,
19:26 

19:26 the more money that service makes. The
19:27 

19:27 whole all these possibilities kind of
19:30 

19:30 they're a little bit scary when you
19:31 

19:31 think about them a little bit deeply.
19:34 

19:34 >> Totally. This is one that really does
19:36 

19:36 that I personally, you know, you can see
19:39 

19:39 the ways that this goes really wrong.
19:40 

19:40 >> Yeah. Uh, you mentioned Enterprise.
19:42 

19:42 Let's talk about Enterprise. you were at
19:44 

19:44 a lunch with some editors and CEOs of
19:47 

19:47 some news companies in New York last
19:49 

19:49 week and told them that enterprise is
19:51 

19:51 going to be a major priority uh for
19:54 

19:54 OpenAI next year.
19:55 

19:55 >> U I'd love to hear a little bit more
19:58 

19:58 about um why that's a priority, how you
20:01 

20:01 think you stack up against anthropic. I
20:03 

20:03 know people will say this is a pivot for
20:06 

20:06 OpenAI that has been consumer focused.
20:08 

20:08 So just give us an overview about the
20:10 

20:10 enterprise plan. Our strategy was always
20:12 

20:12 consumer first. Uh there were a few
20:14 

20:14 reasons for that. One, the models were
20:16 

20:16 not robust and skilled enough uh for
20:20 

20:20 most enterprise uses and now now they're
20:22 

20:22 they're getting there. The second was we
20:24 

20:24 had this like clear opportunity to win
20:26 

20:26 in consumer and those are rare and hard
20:29 

20:29 to come by and I think if you win in
20:30 

20:30 consumer it makes it massively easier to
20:32 

20:32 win in enterprise and we are we are
20:35 

20:35 seeing that now. Um but as I mentioned
20:37 

20:37 earlier this was a year where we
20:39 

20:39 enterprise growth outpaced consumer
20:41 

20:41 growth. Uh and given where the models
20:44 

20:44 are today where they will get to next
20:46 

20:46 year we think this is the time where we
20:48 

20:48 can
20:51 

20:51 build a really significant enterprise
20:53 

20:53 business quite rapidly. I mean I think
20:56 

20:56 and we already have one but it can it
20:58 

20:58 can grow much more. Um
21:00 

21:00 companies seem ready for it. The
21:02 

21:02 technology seems ready for it. the, you
21:05 

21:05 know, coding is the biggest example so
21:08 

21:08 far, but there are others that are now
21:11 

21:11 growing, other verticals that are now
21:12 

21:12 growing very quickly. And we're starting
21:14 

21:14 to hear enterprises say, you know, I
21:16 

21:16 really just want an AI platform.
21:18 

21:18 >> Which vertical company?
21:19 

21:19 >> Um, finance science is the one I'm most
21:23 

21:23 excited about of everything happening
21:25 

21:25 right now. Personally, um, customer
21:27 

21:27 support is doing great. Uh
21:32 

21:32 but but yeah the the
21:36 

21:36 we have this thing called GDP though.
21:38 

21:38 >> I was going to ask you about that. Can I
21:39 

21:39 actually throw my question out about
21:40 

21:40 that? All right. Cuz I wrote to Aaron
21:42 

21:42 Levy the CEO of Box and I said I'm going
21:44 

21:44 to meet with Sam. What should I ask him?
21:46 

21:46 He goes throw a question out about GDP
21:48 

21:48 val. Right. So this is the measure of
21:49 

21:49 how AI performs in knowledge work tasks.
21:52 

21:52 And I said okay. I went back to the
21:53 

21:53 release of GPT 5.2 to the model that uh
21:56 

21:56 you recently released and looked at the
21:59 

21:59 GDP valid chart. Now this of course is
22:00 

22:00 an open AI evaluation. Um that being
22:03 

22:03 said the uh GPT5 thinking model so this
22:07 

22:07 is the model released in the in the
22:09 

22:09 summer. It ti it tied uh knowledge
22:12 

22:12 workers at 38% of test or tied beat or
22:15 

22:15 tied
22:16 

22:16 >> um GP so 38.8% GPT 5.2 2 thinking beat
22:22 

22:22 or tied at 70.9%
22:25 

22:25 of knowledge work tasks and GPT 5.2 pro
22:30 

22:30 74.1%
22:31 

22:31 of knowledge work tasks and it passed
22:33 

22:33 the threshold of um being expert level
22:37 

22:37 it it handled it looks like something
22:38 

22:38 like 60% of expert tasks uh of tasks
22:42 

22:42 that would make it you know on par with
22:43 

22:43 an expert in the knowledge work. What
22:45 

22:45 are the implications of the fact that
22:47 

22:47 these models can do that much knowledge
22:49 

22:49 work? So, you know, you were asking
22:51 

22:51 about verticals, and I think that's a
22:52 

22:52 great question, but the thing that was
22:53 

22:53 going through my mind and why I kind of
22:54 

22:54 was stumbling a little bit is that Eval,
22:57 

22:57 I think it's like 40 something different
22:59 

22:59 verticals that a business has to do.
23:02 

23:02 >> There's make a PowerPoint, do this legal
23:04 

23:04 analysis, you know, write up this little
23:06 

23:06 web app, all this stuff.
23:08 

23:08 >> And and the eval is do experts prefer
23:12 

23:12 the output of the model relative to
23:14 

23:14 other experts
23:17 

23:17 for a lot of the things that a business
23:18 

23:18 has to do. Now, these are small, well
23:20 

23:20 scopeed tasks. These don't get the kind
23:22 

23:22 of complicated, open-ended, creative
23:24 

23:24 work that, you know, figuring out a new
23:26 

23:26 product. These don't get a lot of
23:28 

23:28 collaborative team things. But
23:31 

23:31 a co-orker that you can assign an hour's
23:34 

23:34 worth of tasks to and get something you
23:36 

23:36 like better back 74 or 70% of time if
23:38 

23:38 you want to pay less is still pretty
23:41 

23:41 extraordinary. If you went back to the
23:43 

23:43 launch of Chat TBT 3 years ago and said
23:46 

23:46 we were going to have that in 3 years,
23:47 

23:47 most people would say absolutely not.
23:49 

23:49 Um, and so as we think about how
23:52 

23:52 enterprises are going to integrate this,
23:54 

23:54 it's no longer like just that it can do
23:56 

23:56 code. It's all of these knowledge work
23:58 

23:58 tasks you can kind of farm out to the
24:01 

24:01 AI. uh and
24:06 

24:06 that's going to take a while to really
24:09 

24:09 kind of figure out how enterprises
24:11 

24:11 integrate with it but should be quite
24:13 

24:13 substantial. I know you're not an
24:14 

24:14 economist, so I'm not going to ask you
24:16 

24:16 like what is the macro impact on jobs,
24:17 

24:17 but let me just read you one uh line
24:19 

24:19 that I heard uh you know in in terms of
24:22 

24:22 how this impacts jobs uh from Blood in
24:24 

24:24 the Machine on Substack. Um this is from
24:27 

24:27 a technical copywriter. They said,
24:29 

24:29 "Chatbots came in and made it so my job
24:31 

24:31 was managing the bots instead of a team
24:33 

24:33 of reps." Okay, that that to me seems
24:36 

24:36 like it's going to happen often. But
24:37 

24:37 then this person continued and said once
24:39 

24:39 the bots were sufficiently trained up to
24:41 

24:41 offer good enough support then I was
24:43 

24:43 out. Um is that is that the is that
24:47 

24:47 going to become more common? Is that
24:48 

24:48 what bad companies are going to do?
24:50 

24:50 Because if you have a human who's going
24:51 

24:51 to be able to sort of orchestrate a
24:54 

24:54 bunch of different bots then you might
24:56 

24:56 want to keep them. I don't know. How do
24:57 

24:57 you think about this? So I I agree with
24:60 

24:60 you that it's clear to see how
25:01 

25:01 everyone's going to be managing like a
25:03 

25:03 lot of AI uh doing different stuff. Um
25:09 

25:09 eventually like any good manager
25:10 

25:10 hopefully your team gets better and
25:11 

25:11 better but you just take on more scope
25:13 

25:13 and more responsibility. I am not I am
25:16 

25:16 not a jobs dumer.
25:18 

25:18 Um short term I have some worry. I think
25:20 

25:20 the transition is likely to be rough uh
25:24 

25:24 in some cases but
25:28 

25:28 we are so deeply wired to care about
25:32 

25:32 other people what other people do. We
25:34 

25:34 are so we seem to be so focused on
25:38 

25:38 relative status and always wanting more
25:40 

25:40 and to be of use and service to express
25:44 

25:44 creative spirit whatever whatever
25:45 

25:45 whatever has driven us this long. I
25:47 

25:47 don't think that's going away. Now I do
25:50 

25:50 think the jobs of the future or I don't
25:52 

25:52 even know if jobs is the right word.
25:53 

25:53 Whatever we're all going to do all day
25:55 

25:55 in 2050 probably looks very different
25:57 

25:57 than it does today. Um
26:00 

26:00 but I but I I don't have any of this
26:02 

26:02 like oh life is going to be without
26:04 

26:04 meaning and the economy is going to
26:05 

26:05 totally break. Like we will find I hope
26:08 

26:08 much more meaning and the economy I
26:10 

26:10 think will significantly change but I I
26:14 

26:14 think you just don't bet against
26:15 

26:15 evolutionary biology. Um
26:18 

26:18 you know I think a lot about how we can
26:20 

26:20 automate all the functions at OpenAI and
26:22 

26:22 then even more than that I think about
26:23 

26:23 like what it means to have an AI CEO of
26:25 

26:25 Open AI. Doesn't bother me. I'm like
26:27 

26:27 thrilled for it. I won't fight it. Uh
26:30 

26:30 like I don't want to be I don't want to
26:31 

26:31 be the person hanging on being like I
26:33 

26:33 can do this better the the handmade way.
26:34 

26:34 >> AI CEO just make a bunch of decisions to
26:36 

26:36 sort of like direct all of our resources
26:39 

26:39 to giving AI more energy and power. It's
26:41 

26:41 like
26:42 

26:42 >> um I mean no you would really put a
26:44 

26:44 guard rail on
26:45 

26:45 >> Yeah. Like obviously you don't want an
26:48 

26:48 AI CEO that is not governed by humans,
26:51 

26:51 but if you think about
26:54 

26:54 if if you think about maybe like
26:58 

26:58 a this is a crazy analogy, but I'll give
27:02 

27:02 it anyway. If you think about a version
27:04 

27:04 where like every person in the world was
27:07 

27:07 effectively on the board of directors of
27:09 

27:09 an AI company and got to, you know, tell
27:13 

27:13 the AI CEO what to do and fire them if
27:16 

27:16 they weren't doing a good job at that
27:17 

27:17 and, you know, got governance on the
27:19 

27:19 decisions, but the AI CEO got to try to
27:21 

27:21 like execute the wishes of the board.
27:24 

27:24 Um,
27:26 

27:26 I think to people of the future that
27:28 

27:28 might seem like quite a reasonable
27:29 

27:29 thing. Okay, so we're going to uh move
27:31 

27:31 to infrastructure in a minute, but
27:32 

27:32 before we leave this section on models
27:34 

27:34 and capabilities, when's GP GPT6 coming?
27:39 

27:39 Um, I expect I don't know when we'll
27:42 

27:42 call a model GPT
27:45 

27:45 6. Uh,
27:47 

27:47 but I would expect new models that are
27:49 

27:49 significant gains from 5.2 in the first
27:52 

27:52 quarter of next year.
27:53 

27:53 >> What does significant gains mean?
27:56 

27:56 I don't have like an eval score in mind
27:58 

27:58 for you yet but uh more enterprise side
28:01 

28:01 of things or definitely both the there
28:04 

28:04 will be a lot of improvements to the
28:06 

28:06 model for consumers uh the main thing
28:09 

28:09 consumers want right now is not more IQ
28:11 

28:11 enterprises still do want more IQ so uh
28:14 

28:14 we'll improve the model in different
28:15 

28:15 ways for the kind of for different uses
28:18 

28:18 but uh I our goal is a model that
28:21 

28:21 everybody likes much better
28:22 

28:22 >> so infrastructure you have 1.4 trillion
28:26 

28:26 thereabouts and commitments uh to build
28:28 

28:28 infrastructure. I've listened to a lot
28:31 

28:31 of what you've said about
28:31 

28:31 infrastructure. Um here are some of the
28:34 

28:34 things you said. If people knew what we
28:36 

28:36 could do with compute, they would want
28:38 

28:38 way way more. You said the gap between
28:40 

28:40 what we could offer today versus 10x
28:43 

28:43 compute and 100x compute is substantial.
28:46 

28:46 Uh what what can you help flesh that out
28:49 

28:49 a little bit? What are you going to do
28:51 

28:51 with uh so much more compute?
28:53 

28:53 Well, I mentioned this earlier a little
28:54 

28:54 bit. The thing I'm personally more
28:56 

28:56 excited, most excited about is to use AI
28:59 

28:59 and lots of compute to discover new
29:00 

29:00 science. I am a believer that scientific
29:03 

29:03 discovery is the high order bit of how
29:05 

29:05 the world gets better for everybody. And
29:07 

29:07 if we can throw huge amounts of compute
29:09 

29:09 at scientific problems and discover new
29:12 

29:12 knowledge, which the tiniest bit is
29:14 

29:14 starting to happen now, it's very early.
29:15 

29:15 These are very small things but you know
29:17 

29:17 my learning in history of this field is
29:19 

29:19 once the squiggles start and it lifts
29:21 

29:21 off the x-axis a little bit we know how
29:22 

29:22 to make that better and better. Um but
29:24 

29:24 that takes huge amounts of compute to
29:26 

29:26 do. So that's one area we're like
29:28 

29:28 throwing lots of AI at discovering new
29:30 

29:30 science curing disease lots of other
29:32 

29:32 things. Um,
29:35 

29:35 a kind of recent cool example here is we
29:38 

29:38 built the Sora Android app using codecs
29:43 

29:43 and
29:45 

29:45 they did it in like less than a month.
29:47 

29:47 They used a huge amount. One of the nice
29:49 

29:49 things about working at OpenAI is you
29:50 

29:50 don't get any limits on codecs. They
29:51 

29:51 used a huge amount of tokens, but they
29:54 

29:54 were able to do what would normally have
29:55 

29:55 taken a lot of people much longer and
29:58 

29:58 Codex kind of mostly did it for us. And
30:02 

30:02 you can imagine that going much further
30:04 

30:04 where entire companies can build their
30:06 

30:06 products using lots of compute.
30:10 

30:10 Um
30:12 

30:12 people have talked a lot about how video
30:14 

30:14 models are going to point towards these
30:16 

30:16 generated real-time generated user user
30:19 

30:19 interfaces. That will take a lot of
30:21 

30:21 compute. Um
30:24 

30:24 enterprises that want to transform their
30:26 

30:26 business will use a lot of compute. uh
30:28 

30:28 doctors that want to offer good
30:31 

30:31 personalized health care that are like
30:32 

30:32 constantly
30:34 

30:34 measuring every sign they can get from
30:36 

30:36 each individual patient. You can imagine
30:38 

30:38 that using a lot of compute. Uh it it's
30:41 

30:41 hard to frame how much
30:44 

30:44 compute we're already
30:46 

30:46 using to generate AI output in the
30:49 

30:49 world. Um but these are horribly rough
30:52 

30:52 numbers. So, uh, and I think it's like
30:54 

30:54 undisiplined to talk this way, but I I
30:56 

30:56 always find these like mental thought
30:58 

30:58 experiments a little bit useful, so
30:59 

30:59 forgive me for the sloppiness. Um, let's
31:03 

31:03 say
31:06 

31:06 that an AI company today might be
31:08 

31:08 generating something on the order of 10
31:11 

31:11 trillion tokens a day out of Frontier
31:13 

31:13 models. Um,
31:16 

31:16 you know, more, but not it's not like a
31:19 

31:19 a quadrillion tokens for anybody, I
31:21 

31:21 don't think. Um
31:25 

31:25 let's say there's 8 billion people in
31:26 

31:26 the world and let's say on average
31:28 

31:28 someone's these are I think totally
31:30 

31:30 wrong but let's say someone the average
31:32 

31:32 number of tokens outputed by a person
31:33 

31:33 per day is like
31:36 

31:36 uh 20,000.
31:40 

31:40 You can then start to and the token you
31:42 

31:42 can to be fair then we have to compare
31:44 

31:44 the output tokens of a model provider
31:46 

31:46 today not not all the tokens consumed
31:47 

31:47 but you can start to look at this and
31:49 

31:49 you can say hm we're going to have these
31:54 

31:54 models at a company be outputting more
31:56 

31:56 tokens per day than all of humanity put
31:59 

31:59 together and then 10 times that and then
32:01 

32:01 100 times that. And you know, in some
32:05 

32:05 sense it's like a really silly
32:06 

32:06 comparison,
32:08 

32:08 but in some sense it gives a magnitude
32:10 

32:10 for like how much of the intellectual
32:13 

32:13 crunching on the planet is like human
32:14 

32:14 brains versus AI brains. And that's kind
32:18 

32:18 of the relative growth rates there are
32:21 

32:21 are interesting. And so I'm wondering
32:24 

32:24 are do you know that there is this
32:26 

32:26 demand to use this compute like
32:28 

32:28 potentially like so for instance would
32:30 

32:30 we have surefires like scientific
32:32 

32:32 breakthroughs if you know open AAI were
32:35 

32:35 to put double the compute towards
32:37 

32:37 science or or with medicine like are
32:40 

32:40 would we have you know that clear
32:42 

32:42 ability to assist doctors like
32:44 

32:44 >> how much of this is sort of uh
32:46 

32:46 supposition of what's to happen versus
32:49 

32:49 clear understanding based off of what
32:51 

32:51 you see today IC
32:52 

32:52 >> everything everything based off what we
32:54 

32:54 see today is that it will happen. It
32:56 

32:56 does not mean some crazy thing can't
32:58 

32:58 happen in the future. Someone could
32:60 

32:60 discover some completely new
33:01 

33:01 architecture and there could be a 10,000
33:03 

33:03 times you know efficiency gain and then
33:05 

33:05 we would have really probably overbuilt
33:07 

33:07 for a while. But everything we see right
33:10 

33:10 now about how quickly the models are
33:12 

33:12 getting better at each new level, how
33:13 

33:13 much more people want to use them, each
33:15 

33:15 time we can bring the cost down, how
33:16 

33:16 much more people really want to use
33:18 

33:18 them. Um,
33:22 

33:22 everything about that indicates
33:26 

33:26 to me that there will be increasing
33:29 

33:29 demand and people using these for
33:32 

33:32 wonderful things, for silly things. Um,
33:35 

33:35 but
33:37 

33:37 it it just so seems like
33:41 

33:41 this is the shape of the future. Um
33:45 

33:45 it's not just like it's not just you
33:48 

33:48 know how many tokens we can do per day.
33:49 

33:49 It's how fast we can do them as these
33:51 

33:51 coding models have gotten better. They
33:52 

33:52 can think for a really long time but you
33:53 

33:53 don't want to wait for a really long
33:54 

33:54 time. So there will be other dimensions.
33:56 

33:56 It will not just be the number of tokens
33:57 

33:57 that that we can do. Um but the demand
34:00 

34:00 for intelligence across a small number
34:03 

34:03 of axes
34:05 

34:05 and what we can do with those you know
34:08 

34:08 if you're like if you have like a really
34:10 

34:10 difficult healthcare problem do you want
34:12 

34:12 to use 5.2 or do you want to use 5.2 pro
34:15 

34:15 even if it takes dramatically more
34:17 

34:17 tokens I'll go with the better model. I
34:18 

34:18 think you will um can let's just try to
34:21 

34:21 go one level deeper. Um
34:24 

34:24 going to the scientific discovery, can
34:26 

34:26 you give an example of like a scientist
34:28 

34:28 it doesn't have to well maybe it's one
34:30 

34:30 that you know today that's like I have
34:32 

34:32 problem X and if I put you know compute
34:35 

34:35 Y towards it I will solve it but I'm not
34:37 

34:37 able to today. There was a thing this
34:39 

34:39 morning on Twitter where a bunch of
34:41 

34:41 mathematicians were saying they were all
34:43 

34:43 like replying to each other's tweets. Uh
34:45 

34:45 they're like I was really skeptical that
34:47 

34:47 LM's were ever going to be good. 5.2 is
34:49 

34:49 the one that crossed the boundary for
34:51 

34:51 me. it did it you know figured out this
34:54 

34:54 it with some help it did this small
34:56 

34:56 proof it it discovered this small thing
34:59 

34:59 but it's this is actually changing my
35:00 

35:00 workflow and then people were piling on
35:02 

35:02 saying yeah me too I mean some people
35:03 

35:03 were saying 5.1 was already there not
35:05 

35:05 many
35:06 

35:06 >> um but
35:08 

35:08 that that was like that's a very recent
35:10 

35:10 example this model's only been out for 5
35:12 

35:12 days or something where people are like
35:14 

35:14 all right you know the mathematic
35:16 

35:16 >> the mathematics research community seems
35:17 

35:17 to say like okay something important
35:19 

35:19 just happened
35:19 

35:19 >> I've seen Greg Brockman has been
35:21 

35:21 highlighting getting all these different
35:22 

35:22 mathematical scientific uses in his feed
35:24 

35:24 and something has clicked I think with
35:27 

35:27 5.2 um among these communities. So it'll
35:31 

35:31 be interesting to see what happens as as
35:32 

35:32 things progress.
35:34 

35:34 >> We don't
35:36 

35:36 like one of the hard parts about compute
35:38 

35:38 >> at this scale is you have to do it so
35:40 

35:40 far in advance. So you know that 1.4
35:43 

35:43 trillion you mentioned we'll spend it
35:44 

35:44 over a very long period of time. I wish
35:45 

35:45 we could do it faster. I think there
35:47 

35:47 would be demand if we could do it
35:48 

35:48 faster. Um, but
35:52 

35:52 it just takes an enormously long time to
35:55 

35:55 build these projects and the energy to
35:58 

35:58 run the data centers and the chips and
36:00 

36:00 the systems and the networking and
36:01 

36:01 everything else. Um, so that will be
36:03 

36:03 over a while, but you know, we
36:06 

36:06 from a year ago to now, we probably
36:07 

36:07 about tripled our compute. We'll triple
36:09 

36:09 our compute again next year, hopefully
36:10 

36:10 again after that. um revenue grows even
36:14 

36:14 a little bit faster than that but it
36:15 

36:15 does roughly track our compute
36:19 

36:19 fleet. Uh so we
36:23 

36:23 we have never yet found a situation
36:25 

36:25 where we can't really well monetize all
36:27 

36:27 the compute we have. Um if we had I
36:29 

36:29 think if we had you know double the
36:30 

36:30 compute we'd be at double the revenue
36:32 

36:32 right now.
36:32 

36:32 >> Okay let's let's talk about numbers
36:34 

36:34 since you brought it up. Um revenue is
36:36 

36:36 growing. uh compute spend is growing but
36:39 

36:39 compute spend still outpaces revenue
36:42 

36:42 growth. Uh I think the numbers that have
36:44 

36:44 been reported are OpenAI is supposed to
36:46 

36:46 lose something like 120 billion between
36:50 

36:50 now and 120 and 2028 29 where you're
36:54 

36:54 going to become profitable. Um so talk a
36:57 

36:57 little bit about like how does that
36:59 

36:59 change? Where does the turn happen? I
37:01 

37:01 mean, as revenue grows and as inference
37:05 

37:05 becomes a larger and larger part of the
37:07 

37:07 fleet, it eventually uh subsumes the
37:10 

37:10 training expense. So, that's the plan.
37:12 

37:12 Spend a lot of money training but make
37:14 

37:14 more and more. Uh if we if we weren't
37:17 

37:17 continuing to grow our training
37:19 

37:19 costs by so much, uh we would be
37:22 

37:22 profitable way way earlier. Um but the
37:26 

37:26 bet we're making is to invest very
37:28 

37:28 aggressively in training these big
37:29 

37:29 models. The whole world is wondering um
37:32 

37:32 how your revenue will line up with the
37:35 

37:35 spend. Uh the question's been asked if
37:38 

37:38 the trajectory is to hit 20 billion in
37:41 

37:41 revenue this year and the the spend
37:43 

37:43 commitment is 1.4 trillion. Um so I
37:47 

37:47 think it would be great just over a very
37:49 

37:49 long period.
37:49 

37:49 >> Yeah. Over and that's why I wanted to
37:51 

37:51 bring it up to you. I think it would be
37:52 

37:52 great to just lay it out for everyone
37:54 

37:54 once and for all how those numbers are
37:56 

37:56 going to work. It's it's very hard to
37:59 

37:59 like really I I I find that one thing I
38:03 

38:03 certainly can't do it and very few
38:04 

38:04 people I've ever met can do it. You
38:06 

38:06 know, you can like you have good
38:08 

38:08 intuition for a lot of mathematical
38:09 

38:09 things in your head, but exponential
38:11 

38:11 growth is usually very hard for people
38:13 

38:13 to do a good quick mental framework on
38:16 

38:16 like for whatever reason there were a
38:18 

38:18 lot of things that evolution needed us
38:20 

38:20 to be able to do well with math in our
38:21 

38:21 heads. Modeling exponential growth
38:24 

38:24 doesn't seem to be one of them. Um so
38:28 

38:28 the thing we believe is that we can stay
38:30 

38:30 on
38:32 

38:32 a very steep
38:36 

38:36 growth curve of revenue for quite a
38:38 

38:38 while and everything we see right now
38:39 

38:39 continues to indicate that we cannot do
38:41 

38:41 it if we don't have the compute. uh
38:43 

38:43 again we're so compute constrained uh
38:45 

38:45 and it hits the revenue line so hard
38:48 

38:48 that I think if we get to a point where
38:51 

38:51 we have like a lot of compute sitting
38:52 

38:52 around that we can't monetize on a you
38:55 

38:55 know profitable per unit of compute
38:56 

38:56 basis be very reasonable to say okay
38:59 

38:59 this is like a little how's this all
39:01 

39:01 going to work but
39:03 

39:03 we've penciled this out a bunch of ways
39:06 

39:06 uh we will of course also get more
39:08 

39:08 efficient uh on like a flops per dollar
39:11 

39:11 basis as you know all of the work we've
39:13 

39:13 been doing to make comput cheaper comes
39:14 

39:14 to pass. Um, but
39:19 

39:19 we see this consumer growth, we see this
39:20 

39:20 enterprise growth. There's a whole bunch
39:22 

39:22 of new kinds of businesses that
39:24 

39:24 have we haven't even launched yet but
39:26 

39:26 will. Um, but compute is really the
39:28 

39:28 lifeblood that enables all of this. So
39:31 

39:31 we, you know, there's like checkpoints
39:33 

39:33 along the way and if we're a little bit
39:34 

39:34 wrong about our timing or math, we can
39:37 

39:37 we have some flexibility, but
39:40 

39:40 we have always been in a comput deficit.
39:43 

39:43 It has always constrained what we're
39:44 

39:44 able to do. Uh I unfortunately think
39:47 

39:47 that will always be the case, but I wish
39:48 

39:48 it were less the case and I'd like to
39:50 

39:50 get it to be less of the case over time.
39:52 

39:52 Uh because I think there's so many great
39:53 

39:53 products and services that we can
39:55 

39:55 deliver and it'll be a great business.
39:57 

39:57 Okay. So, it's effectively training
39:59 

39:59 costs go down
39:60 

39:60 >> as a percentage go up overall. But yeah,
40:03 

40:03 >> and then your expectation is through
40:05 

40:05 things like this this enterprise push
40:07 

40:07 through things like people being willing
40:09 

40:09 uh to pay for chat GPT through the API,
40:12 

40:12 OpenAI will be able to grow revenue
40:14 

40:14 enough to pay for it with revenue.
40:16 

40:16 >> Yeah, that is the plan.
40:18 

40:18 >> Now, I think the thing so the market's
40:20 

40:20 been kind of losing its mind over this
40:23 

40:23 um recently. I think the thing that has
40:25 

40:25 spooked the market has been the debt has
40:27 

40:27 entered uh into this equation. And the
40:31 

40:31 idea around debt is you take debt out
40:33 

40:33 when there's something that's
40:34 

40:34 predictable. Um and then companies will
40:37 

40:37 take the debt out, they'll build and
40:38 

40:38 they'll have predictable revenue.
40:40 

40:40 >> But it's it's the this is a new
40:43 

40:43 category. It's it is unpredictable. Um
40:46 

40:46 is is that how do you think about the
40:47 

40:47 fact that that debt has entered the
40:49 

40:49 picture here? So, first of all, I think
40:51 

40:51 the market more lost its mind when
40:55 

40:55 earlier this year, you know, we would
40:56 

40:56 like meet with some company and that
40:58 

40:58 company's stock would go up 20% or 15%
40:60 

40:60 the next day. That was crazy.
41:01 

41:01 >> That felt really unhealthy. Um, I'm
41:04 

41:04 actually happy that there's like a
41:06 

41:06 little bit more skepticism and
41:08 

41:08 rationality in the market now cuz uh it
41:10 

41:10 felt to me like we were just totally
41:12 

41:12 heading towards a very unstable bubble
41:14 

41:14 and now I think people are some degree
41:17 

41:17 of discipline. So I actually think
41:18 

41:18 things are I think people went crazy
41:20 

41:20 earlier and now people are being more
41:21 

41:21 rational on the debt front. I I think we
41:27 

41:27 do kind of we know that if we build
41:31 

41:31 infrastructure we the industry someone's
41:33 

41:33 going to get value out of it. And it's
41:37 

41:37 still it's still totally early. I agree
41:40 

41:40 with you. But I don't think anyone's
41:42 

41:42 still questioning there's not going to
41:43 

41:43 be value from like AI infrastructure.
41:46 

41:46 And so I think it is reasonable for debt
41:49 

41:49 to
41:52 

41:52 enter this market. I think there will
41:53 

41:53 also be other kinds of financial
41:54 

41:54 instruments. I suspect we'll see some
41:56 

41:56 unreasonable ones as people really you
41:59 

41:59 know innovate about how to finance this
42:02 

42:02 sort of stuff. But you know like lending
42:04 

42:04 companies money to build data centers
42:06 

42:06 that that seems fine to me. I think the
42:07 

42:07 the fear is that um if things don't
42:10 

42:10 continue at pace like here's one
42:11 

42:11 scenario um and you'll probably disagree
42:15 

42:15 with this but like the model progress
42:16 

42:16 saturates uh then the the infrastructure
42:20 

42:20 becomes worth less than the anticipated
42:22 

42:22 value was and then yes those data
42:25 

42:25 centers will be worth something to
42:26 

42:26 someone but it could be that they get
42:28 

42:28 liquidated and someone buys them at a
42:30 

42:30 discount. Yeah. And and I do suspect by
42:32 

42:32 the way there will be some like booms
42:33 

42:33 and busts along the way. These things
42:35 

42:35 are never a perfectly smooth line. Um,
42:39 

42:39 first of all, it seems very clear to me,
42:41 

42:41 and this is like a thing I happily would
42:43 

42:43 bet the company on, that the models are
42:45 

42:45 going to get much much better. We have
42:47 

42:47 like a pretty good window into this.
42:48 

42:48 We're very confident about that. Even if
42:50 

42:50 they did not, I think the
42:54 

42:54 there's like a lot of inertia in the
42:55 

42:55 world. It takes a while to figure out
42:57 

42:57 how to adapt to things. The overhang of
42:60 

42:60 the economic value that I believe 5.2 2
43:03 

43:03 represents relative to what the world
43:05 

43:05 has figured out how to get out of it so
43:06 

43:06 far is so huge that even if you froze
43:09 

43:09 the model at 5.2 to how much more like
43:12 

43:12 value can you create and thus revenue
43:13 

43:13 can you drive? I bet a huge amount. In
43:16 

43:16 fact, you didn't ask this, but if I can
43:18 

43:18 go on a rant for a second. Um,
43:22 

43:22 we used to talk a lot about this 2x2
43:24 

43:24 matrix of short timelines,
43:27 

43:27 long timelines, slow takeoff, fast
43:28 

43:28 takeoff, and where we felt at different
43:31 

43:31 times the kind of probability was
43:32 

43:32 shifting, and that that was going to be
43:34 

43:34 you could kind of understand a lot of
43:36 

43:36 the decisions and strategy that the
43:39 

43:39 world should optimize for based off of
43:41 

43:41 where you were going to be on that 2x
43:42 

43:42 two matrix. Um,
43:48 

43:48 there's like a Z-axis in my head in my
43:50 

43:50 picture of this that's emerged, which is
43:53 

43:53 small overhang, big overhang. And
43:57 

43:57 I I kind of thought that
44:00 

44:00 I guess I didn't think about that hard,
44:02 

44:02 but uh like my retro on this is I must
44:05 

44:05 have assumed that the overhang was not
44:06 

44:06 going to be that massive that if the
44:08 

44:08 models had a lot of value in them, the
44:11 

44:11 world was pretty quickly going to figure
44:12 

44:12 out how to deploy that. But it looks to
44:15 

44:15 me now like the overhang is going to be
44:16 

44:16 massive in most of the world. You'll
44:18 

44:18 have these like areas like you know some
44:20 

44:20 some set of coders that'll get massively
44:22 

44:22 more productive by adopting these tools.
44:25 

44:25 But on the whole
44:27 

44:27 you have this crazy smart model that to
44:30 

44:30 be perfectly honest most people are
44:32 

44:32 still asking this similar questions they
44:33 

44:33 did in the GPD4 realm. Scientists
44:36 

44:36 different coders different maybe
44:37 

44:37 knowledge work is going to get different
44:39 

44:39 but but there is a huge overhang and
44:43 

44:43 that has a bunch of very strange
44:44 

44:44 consequences for the world. I we have
44:46 

44:46 not wrapped our head around all the ways
44:48 

44:48 that's going to play out yet, but is
44:50 

44:50 very much not what I would have expected
44:51 

44:51 a few years ago. I have a question for
44:53 

44:53 you about this uh capability overhang.
44:55 

44:55 Basically, the models can do a lot more
44:57 

44:57 than they've been doing. Um I I'm trying
45:00 

45:00 to figure out how um the models can be
45:03 

45:03 that much better than they're being used
45:05 

45:05 for, but a lot of businesses when they
45:07 

45:07 try to implement them, they're not
45:09 

45:09 getting a return on their investment.
45:11 

45:11 >> Um or at least that's what they tell
45:13 

45:13 MIT. I'm not sure quite how to think
45:15 

45:15 about that because we hear all these
45:16 

45:16 businesses saying, you know, if you 10x
45:19 

45:19 the price of GPT 5.2, we would still pay
45:21 

45:21 for it. Like you're hugely underpricing
45:23 

45:23 this, we're getting all this value out
45:24 

45:24 of it.
45:24 

45:24 >> Um,
45:26 

45:26 so I don't that doesn't seem right to
45:29 

45:29 me. Certainly, if you talk about like
45:31 

45:31 what coders say, they're like, "This is,
45:33 

45:33 you know, I'd pay 100 times the price or
45:36 

45:36 whatever." Um,
45:36 

45:36 >> just be bureaucracy that's messing
45:38 

45:38 things up. Let's say you believe the GDP
45:40 

45:40 valve numbers and maybe you don't for
45:42 

45:42 good reason maybe they're wrong but let
45:43 

45:43 let's say it were true and for kind of
45:46 

45:46 these wellsp specified not super long
45:49 

45:49 duration knowledge work tasks seven out
45:51 

45:51 of 10 times you would be as happy or
45:54 

45:54 happier with the 5.2 output.
45:57 

45:57 You should then be using that a lot. And
45:60 

45:60 yet it takes people so long to change
46:01 

46:01 their workflow. are so used to asking
46:03 

46:03 the junior analyst to make a deck or
46:06 

46:06 whatever that they're going to like it
46:10 

46:10 just that's stickier than I thought it
46:12 

46:12 was. You know, I still kind of run my
46:15 

46:15 workflow in very much the same way.
46:18 

46:18 Although I know that I could be using AI
46:19 

46:19 much more than I am. Yep. All right, we
46:22 

46:22 got 10 minutes left. I got Wow, that was
46:23 

46:23 quick. I got four questions. Uh let's
46:25 

46:25 see if we can lightning round uh through
46:27 

46:27 them. So, uh, the device that you're
46:30 

46:30 working on. We'll be back with OpenAI
46:33 

46:33 CEO Sam Alman right after this. Um, what
46:36 

46:36 I've heard, phone size, no screen. Um,
46:41 

46:41 why couldn't it be an app if it's the
46:43 

46:43 phone if it's the phone without a
46:46 

46:46 screen? First, we're going to do a f a
46:47 

46:47 small family of devices. It will not be
46:49 

46:49 a single device. uh there will be over
46:51 

46:51 time a
46:55 

46:55 this is this is not speculation so I'm
46:56 

46:56 may try not to be totally wrong but I
46:58 

46:58 think there will be a shift over time to
46:60 

46:60 the way people use computers where they
47:03 

47:03 go from a sort of
47:06 

47:06 dumb reactive thing to a very smart
47:10 

47:10 proactive thing that is understanding
47:11 

47:11 your whole life your context everything
47:12 

47:12 going on around you very aware of
47:16 

47:16 the people around you physically or
47:19 

47:19 close to you via a computer that you're
47:23 

47:23 working with. And I don't think current
47:26 

47:26 devices are well suited
47:30 

47:30 to that kind of world. And I am a big
47:32 

47:32 believer that we like we work at the
47:34 

47:34 limit of our devices. you know, you have
47:38 

47:38 that computer and it has a bunch of
47:41 

47:41 design choices. Like it could be open or
47:43 

47:43 closed, but it can't be, you know,
47:45 

47:45 there's not like a okay, pay attention
47:47 

47:47 to this interview, but be closed and
47:50 

47:50 like whisper in my ear if I forget to
47:51 

47:51 ask Sam a question or whatever. Um,
47:54 

47:54 >> maybe that would be helpful. And there's
47:56 

47:56 like, you know, there's like
47:58 

47:58 a screen and that like limits you to the
48:01 

48:01 kind of same way we've had graphical
48:04 

48:04 user interfaces working for many
48:05 

48:05 decades. And there's,
48:07 

48:07 you know, a keyboard that was built to
48:09 

48:09 like slow down how fast you could get
48:11 

48:11 information into it. And these have just
48:14 

48:14 been unquestioned assumptions for a long
48:15 

48:15 time, but they worked. And then this
48:17 

48:17 totally new thing came along and it
48:21 

48:21 opens up a possibility space. But
48:24 

48:24 I don't think the current form factor of
48:28 

48:28 devices is the optimal fit. It'd be very
48:31 

48:31 odd if it were for this like incredible
48:33 

48:33 new affordance we have. Oh man, we could
48:35 

48:35 talk for an hour about this, but um
48:37 

48:37 let's move on to the next one. Cloud.
48:39 

48:39 You've talked about building a cloud. Um
48:42 

48:42 here's a an email we got from a
48:44 

48:44 listener. At my company, we're moving
48:47 

48:47 off Azure and directly integrating with
48:49 

48:49 OpenAI to power our AI experiences in
48:53 

48:53 the product. The focus is to insert a
48:55 

48:55 stream of trillions of tokens powering
48:58 

48:58 AI experiences through the stack. Is is
49:01 

49:01 that the plan to build a big big cloud
49:03 

49:03 business in that in that way?
49:05 

49:05 >> First of all, trillions of tokens, a lot
49:06 

49:06 of tokens. And if you know you asked
49:08 

49:08 about the need for compute and our
49:09 

49:09 enterprise strategy like
49:12 

49:12 >> enterprises have been clear with us
49:14 

49:14 about how many tokens they'd like to buy
49:15 

49:15 from us and we are going to again fail
49:18 

49:18 in 2026 to meet demand but the strategy
49:21 

49:21 is companies
49:23 

49:23 most companies seem to want to come to a
49:26 

49:26 company like us and say I'd like the
49:28 

49:28 name of my company with AI. I need an
49:31 

49:31 API customized for my company. I need
49:33 

49:33 Chach Enterprise customized for my
49:35 

49:35 company. I need a platform that can like
49:37 

49:37 run all these agents that I can trust my
49:38 

49:38 data on. I need the ability to get
49:40 

49:40 trillions of tokens into my product. I
49:42 

49:42 need the ability to have all my internal
49:45 

49:45 processes be more efficient and
49:51 

49:51 we don't currently have like a great
49:52 

49:52 all-in-one offering for them and we'd
49:54 

49:54 like to make that.
49:55 

49:55 >> Is your ambition to put it up there with
49:57 

49:57 the AWS and Ashers of the world?
49:59 

49:59 >> Uh I think it's I think it's a different
50:01 

50:01 kind of thing than those. like I don't I
50:04 

50:04 don't really have an ambition to go
50:07 

50:07 offer whatever all the services you have
50:09 

50:09 to offer to host a website or I don't
50:11 

50:11 even know but uh but I I I think the
50:13 

50:13 concept
50:16 

50:16 yeah my my guess is that people will
50:19 

50:19 continue to have their
50:22 

50:22 call it web cloud and then I think there
50:25 

50:25 will be this other thing where like a
50:27 

50:27 company will be like I need an AI
50:28 

50:28 platform for everything that I want to
50:30 

50:30 do internally the service I want to
50:31 

50:31 offer whatever
50:33 

50:33 and you know like it does kind of live
50:35 

50:35 on the physical hardware in some sense
50:37 

50:37 but
50:38 

50:38 I think it'll be a fairly different
50:40 

50:40 product offering. Uh let's talk about
50:41 

50:41 discovery quickly. Um you've said
50:44 

50:44 something that's been really interesting
50:45 

50:45 to me uh you that you think that the
50:48 

50:48 models or maybe it's people working with
50:50 

50:50 models or the models will make small
50:51 

50:51 discoveries next year and big ones
50:53 

50:53 within five. Is that the models? Is it
50:56 

50:56 people working alongside them? And what
50:58 

50:58 makes you confident that that's going to
50:59 

50:59 happen? Yeah, people using the models
51:01 

51:01 like the the models that can like figure
51:04 

51:04 out their own questions to ask that does
51:05 

51:05 feel further off. But if the world is
51:08 

51:08 benefiting from new knowledge like we
51:11 

51:11 should be very thrilled and you know
51:13 

51:13 like I think the the whole course of
51:18 

51:18 human progress has been that we build
51:19 

51:19 these better tools and then people use
51:21 

51:21 them to do more things and then out of
51:23 

51:23 that process they build more tools and
51:24 

51:24 it's this like scaffolding that we climb
51:26 

51:26 like layer by layer, generation by
51:28 

51:28 generation, discovery by discovery and
51:30 

51:30 the fact that a human's asking the
51:33 

51:33 question I think in no way diminishes
51:35 

51:35 the value of the tool. All right. So, I
51:36 

51:36 I think it's great. I'm all happy. Um
51:40 

51:40 I at the beginning of this year, I
51:42 

51:42 thought the small discoveries were going
51:43 

51:43 to start in 2026. They started in 2025
51:45 

51:45 in late 2025. Again, these are very
51:47 

51:47 small. I really don't want to overstate
51:48 

51:48 them, but
51:51 

51:51 anything
51:53 

51:53 is feels qualitatively to me very
51:55 

51:55 different than nothing. And certainly in
51:58 

51:58 the when we launched three years ago,
51:60 

52:00 that model was not going to make any new
52:02 

52:02 contribution to the total of human
52:03 

52:03 knowledge. um
52:09 

52:09 what it looks like from here to five
52:10 

52:10 years from now. This journey to big
52:12 

52:12 discoveries, I suspect it's just like
52:14 

52:14 like the normal hill climb of AI. It
52:16 

52:16 just gets like a little bit better every
52:18 

52:18 quarter and then all of a sudden we're
52:20 

52:20 like, whoa, humans augmented by these
52:23 

52:23 models are doing things that humans 5
52:25 

52:25 years ago just absolutely couldn't do.
52:28 

52:28 And
52:30 

52:30 you know, whether we mostly attribute
52:32 

52:32 that to smarter humans or smarter
52:34 

52:34 models, as long as we get the scientific
52:35 

52:35 discoveries, I'm very happy either way.
52:38 

52:38 IPO next year. I don't know. Do you want
52:41 

52:41 to be a public company?
52:44 

52:44 >> Um, you seem like you can operate
52:46 

52:46 private for a long time. Would you go
52:48 

52:48 before you needed to
52:52 

52:52 terms of funding?
52:52 

52:52 >> There's like a whole bunch of things at
52:53 

52:53 play here. I do think it's cool that
52:58 

52:58 public markets get to participate in
52:60 

52:60 value creation and you know in some
53:03 

53:03 sense we will be very late to go public
53:06 

53:06 if you look at any previous company. Um
53:10 

53:10 it's wonderful to be a private company.
53:12 

53:12 Uh we need lots of capital. Uh
53:16 

53:16 we're going to you know cross all of the
53:18 

53:18 sort of shareholder limits and stuff at
53:20 

53:20 some point. So,
53:22 

53:22 am I excited
53:24 

53:24 to be a public company CEO? 0%. Um, am I
53:30 

53:30 excited for Open Eye to be a public
53:31 

53:31 company? In some ways, I am. And in some
53:34 

53:34 ways, I think it'll be really annoying.
53:37 

53:37 I listened to your Theo van interview
53:39 

53:39 very closely. Uh, great interview.
53:42 

53:42 >> He was really cool.
53:43 

53:43 >> Theo really knows what he's talking.
53:46 

53:46 He's
53:46 

53:46 >> He did his homework. You told him, this
53:49 

53:49 was right before GPT5 came out, that
53:51 

53:51 GPT5 is smarter than us in almost every
53:54 

53:54 way. Uh, I I thought that that was the
53:58 

53:58 definition of AGI. Does is that isn't
54:00 

54:00 that AGI? And and if not, has the term
54:03 

54:03 become somewhat meaningless? These
54:05 

54:05 models are clearly extremely smart on a
54:08 

54:08 sort of raw horsepower basis. You know,
54:10 

54:10 there's all this stuff on the last
54:11 

54:11 couple of days about GPT 5.2 who has an
54:13 

54:13 IQ of 147 or 144 or 151 or whatever it
54:18 

54:18 is. It's like, you know, depending on
54:20 

54:20 whose test it's like it's some high
54:22 

54:22 number and you have like a lot of
54:24 

54:24 experts in their field saying
54:28 

54:28 it can do these amazing things and it's
54:29 

54:29 like contributing it's making it more
54:31 

54:31 effective. You have the GDP things we
54:32 

54:32 talked about. One thing you don't have
54:36 

54:36 is
54:38 

54:38 the ability for the model to not be able
54:40 

54:40 to do something today, realize it can't
54:43 

54:43 go off and figure out how to learn to
54:45 

54:45 get good at that thing, learn to
54:46 

54:46 understand it, and when you come back
54:47 

54:47 the next day, it gets it right. And that
54:50 

54:50 kind of continuous learning like
54:55 

54:55 toddlers can do it. It does seem to me
54:58 

54:58 like an important part of what we need
55:01 

55:01 to build. Now, can you have something
55:03 

55:03 that most people would consider an AGI
55:04 

55:04 without that? I would say clear. I mean,
55:06 

55:06 there's a lot of people that would say
55:08 

55:08 we're at AGI with our current models.
55:10 

55:10 Um,
55:13 

55:13 I think almost everyone would agree that
55:14 

55:14 if we were at the current level of
55:16 

55:16 intelligence and had that other thing,
55:17 

55:17 it would clearly be very AGI like. Um,
55:21 

55:21 but maybe most of the world will say,
55:26 

55:26 "Okay, fine. Even without that, like
55:27 

55:27 it's doing most knowledge tasks that
55:29 

55:29 matter. um smarter than us in mo most of
55:32 

55:32 us in most ways. We're at AGI. You know,
55:34 

55:34 it's discovering small piece of new
55:35 

55:35 science. We're at AGI. What I think this
55:38 

55:38 means is that the term although it's
55:40 

55:40 been very hard for all of us to stop
55:41 

55:41 using is very underdefined, right?
55:45 

55:45 I I have a I have a a can like one thing
55:49 

55:49 I would love
55:51 

55:51 since we got wrong with AGI. We never
55:53 

55:53 define that that you know the new term
55:54 

55:54 everyone's focused about is when we get
55:55 

55:55 to super intelligence. Um so my proposal
55:59 

55:59 is that we agree that you know AGI kind
56:02 

56:02 of went whooshing by. It was didn't
56:05 

56:05 change the world that much or it will in
56:08 

56:08 the long term but okay fine we've built
56:10 

56:10 AGIs at some point you know we're in
56:12 

56:12 this like fuzzy period where some people
56:14 

56:14 think we have and some people think we
56:16 

56:16 have and more people will think we have
56:17 

56:17 and and then we'll say okay what's next?
56:19 

56:19 Um, a candidate definition for super
56:23 

56:23 intelligence is when a system can do a
56:26 

56:26 better job being president of United
56:29 

56:29 States, CEO of a major company, you
56:32 

56:32 know, running a very large scientific
56:33 

56:33 lab than any person can even with the
56:37 

56:37 assistance of AI.
56:39 

56:39 >> Okay,
56:40 

56:40 >> I think this was an interesting thing
56:41 

56:41 about what happened with chess where
56:44 

56:44 chess got it could be humans. I remember
56:47 

56:47 this very vividly. uh that deep blue
56:49 

56:49 thing and then there was a period of
56:51 

56:51 time where
56:53 

56:53 a human and the AI together were better
56:56 

56:56 than an AI by itself and then the person
56:60 

57:00 was just making it worse and the
57:02 

57:02 smartest thing was the unaded AI that
57:04 

57:04 didn't have the human like
57:07 

57:07 not understanding its its great
57:09 

57:09 intelligence. Um
57:12 

57:12 I think something like that is like an
57:13 

57:13 interesting framework for super
57:15 

57:15 intelligence. I think it's like a long
57:16 

57:16 way off, but I would love to have like a
57:18 

57:18 cleaner definition this time around.
57:20 

57:20 >> Well, Sam, look, I I have uh been in
57:22 

57:22 your products uh using them daily for 3
57:25 

57:25 years. Um
57:27 

57:27 >> definitely gotten a lot better. Can't
57:29 

57:29 even imagine where they go from here.
57:30 

57:30 >> We'll we'll try to keep getting them
57:32 

57:32 better fast.
57:32 

57:32 >> Okay. And uh this is our second time
57:35 

57:35 speaking and I appreciate how open
57:37 

57:37 you've been uh both times. So, thank you
57:38 

57:38 for your time.
57:40 

57:40 >> Thank you everybody for listening and
57:41 

57:41 watching. If you're here for the first
57:43 

57:43 time, please hit follow or subscribe. We
57:46 

57:46 have lots of great interviews on the
57:47 

57:47 feed and more on the way. This past
57:49 

57:49 year, we've had Google DeepMind CEO
57:51 

57:51 Demisabus on twice, including one with
57:54 

57:54 Google founder Sergey Brin. We've also
57:56 

57:56 had Dario Ammoday, the CEO of Anthropic.
57:60 

57:60 And we have plenty of big interviews
58:02 

58:02 coming up in 2026. Thanks again, and
58:04 

58:04 we'll see you next time on Big
58:06 

58:06 Technology Podcast.